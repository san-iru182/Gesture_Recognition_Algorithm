{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import math as math\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader,Subset\n",
        "from torchvision import transforms, utils\n",
        "from torch import nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "JqVojuZQU8Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load data into your Colab notebook\n"
      ],
      "metadata": {
        "id": "YiXfNugkI4FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1M4qlHTiexWQH_crbPY8LiQOqC2ab_yxA\n",
        "! unzip Archive.zip"
      ],
      "metadata": {
        "id": "G3O-u-upI6nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "bdc30ee0-a04c-43c5-8ad4-95b728a321d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1M4qlHTiexWQH_crbPY8LiQOqC2ab_yxA\n",
            "To: /content/Archive.zip\n",
            "\r  0% 0.00/812k [00:00<?, ?B/s]\r100% 812k/812k [00:00<00:00, 9.09MB/s]\n",
            "Archive:  Archive.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: __MACOSX/._test.csv     \n",
            "  inflating: 230013130014_P104_standardized_EMG_two-handed-tap_1_dollar.csv  \n",
            "  inflating: __MACOSX/._230013130014_P104_standardized_EMG_two-handed-tap_1_dollar.csv  \n",
            "  inflating: 230013130022_P104_standardized_EMG_two-handed-tap_2_dollar.csv  \n",
            "  inflating: __MACOSX/._230013130022_P104_standardized_EMG_two-handed-tap_2_dollar.csv  \n",
            "  inflating: 230013130029_P104_standardized_EMG_two-handed-tap_3_dollar.csv  \n",
            "  inflating: __MACOSX/._230013130029_P104_standardized_EMG_two-handed-tap_3_dollar.csv  \n",
            "  inflating: 230113130139_P104_standardized_EMG_point-and-pinch_1_dollar.csv  \n",
            "  inflating: __MACOSX/._230113130139_P104_standardized_EMG_point-and-pinch_1_dollar.csv  \n",
            "  inflating: 230113130145_P104_standardized_EMG_point-and-pinch_2_dollar.csv  \n",
            "  inflating: __MACOSX/._230113130145_P104_standardized_EMG_point-and-pinch_2_dollar.csv  \n",
            "  inflating: 230113130151_P104_standardized_EMG_point-and-pinch_3_dollar.csv  \n",
            "  inflating: __MACOSX/._230113130151_P104_standardized_EMG_point-and-pinch_3_dollar.csv  \n",
            "  inflating: 230313130319_P104_standardized_EMG_pinch-and-scroll_1_dollar.csv  \n",
            "  inflating: __MACOSX/._230313130319_P104_standardized_EMG_pinch-and-scroll_1_dollar.csv  \n",
            "  inflating: 230313130326_P104_standardized_EMG_pinch-and-scroll_2_dollar.csv  \n",
            "  inflating: __MACOSX/._230313130326_P104_standardized_EMG_pinch-and-scroll_2_dollar.csv  \n",
            "  inflating: 230313130332_P104_standardized_EMG_pinch-and-scroll_3_dollar.csv  \n",
            "  inflating: __MACOSX/._230313130332_P104_standardized_EMG_pinch-and-scroll_3_dollar.csv  \n",
            "  inflating: 230513130511_P104_standardized_EMG_air-tap_1_dollar.csv  \n",
            "  inflating: __MACOSX/._230513130511_P104_standardized_EMG_air-tap_1_dollar.csv  \n",
            "  inflating: 230513130516_P104_standardized_EMG_air-tap_2_dollar.csv  \n",
            "  inflating: __MACOSX/._230513130516_P104_standardized_EMG_air-tap_2_dollar.csv  \n",
            "  inflating: 230513130520_P104_standardized_EMG_air-tap_3_dollar.csv  \n",
            "  inflating: __MACOSX/._230513130520_P104_standardized_EMG_air-tap_3_dollar.csv  \n",
            "  inflating: 235813125813_P104_standardized_EMG_palm-pinch_1_dollar.csv  \n",
            "  inflating: __MACOSX/._235813125813_P104_standardized_EMG_palm-pinch_1_dollar.csv  \n",
            "  inflating: 235813125822_P104_standardized_EMG_palm-pinch_2_dollar.csv  \n",
            "  inflating: __MACOSX/._235813125822_P104_standardized_EMG_palm-pinch_2_dollar.csv  \n",
            "  inflating: 235813125828_P104_standardized_EMG_palm-pinch_3_dollar.csv  \n",
            "  inflating: __MACOSX/._235813125828_P104_standardized_EMG_palm-pinch_3_dollar.csv  \n",
            "  inflating: train.csv               \n",
            "  inflating: __MACOSX/._train.csv    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split up test and train data\n",
        "train = pd.read_csv(\"train.csv\").to_numpy()\n",
        "test = pd.read_csv(\"test.csv\").to_numpy()"
      ],
      "metadata": {
        "id": "2MjLGbU0UmYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BELOW THIS IS THE CODE TO EDIT.\n",
        "I have left skeleton code for you to edit."
      ],
      "metadata": {
        "id": "I_pjtSQiJAP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deep-learning\n",
        "\n",
        "- [Documentation for 1D CNN](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
        "- [Documentation for LSTM](https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "- [Documentation for FFNN](https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)"
      ],
      "metadata": {
        "id": "s8wSBYM7NViO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "\"\"\"\n",
        "deep_learning\n",
        "  code skeleton for deep_learning algorithm\n",
        "  inputs:\n",
        "      train: training data [class name x data (64 x 88)]\n",
        "      test: testing data [class name x data (64 x 88)]\n",
        "\n",
        "\"\"\"\n",
        "def deep_learning(train, test):\n",
        "\n",
        "    # Define search space for hyperparameters\n",
        "    cnn_out_channels_list = [8, 16, 32]\n",
        "    kernel_size_list = [3, 5, 7]\n",
        "    hidden_size_list = [32, 64, 128]\n",
        "    dropout_list = [0.0, 0.2, 0.5]\n",
        "    num_epochs_list = [10, 20]\n",
        "    batch_size_list = [2, 4]\n",
        "\n",
        "    training_data = BiosignalDataset(csv_file=\"train.csv\")\n",
        "    test_data = BiosignalDataset(csv_file=\"test.csv\")\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_params = None\n",
        "\n",
        "    # Try all hyperparameter combinations\n",
        "    for out_channels, kernel_size, hidden_size, dropout, num_epochs, batch_size in product(\n",
        "        cnn_out_channels_list, kernel_size_list, hidden_size_list,\n",
        "        dropout_list, num_epochs_list, batch_size_list\n",
        "    ):\n",
        "\n",
        "\n",
        "        model = NeuralNetwork(\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            hidden_size=hidden_size,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        _, test_acc = vanillaNN(training_data, test_data, batch_size, num_epochs, model)\n",
        "\n",
        "\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_params = {\n",
        "                \"out_channels\": out_channels,\n",
        "                \"kernel_size\": kernel_size,\n",
        "                \"hidden_size\": hidden_size,\n",
        "                \"dropout\": dropout,\n",
        "                \"num_epochs\": num_epochs,\n",
        "                \"batch_size\": batch_size\n",
        "            }\n",
        "\n",
        "    print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "    # Train final model with best hyperparameters\n",
        "    final_model = NeuralNetwork(\n",
        "        out_channels=best_params[\"out_channels\"],\n",
        "        kernel_size=best_params[\"kernel_size\"],\n",
        "        hidden_size=best_params[\"hidden_size\"],\n",
        "        dropout=best_params[\"dropout\"]\n",
        "    )\n",
        "\n",
        "    _, test_acc = vanillaNN(\n",
        "        training_data,\n",
        "        test_data,\n",
        "        best_params[\"batch_size\"],\n",
        "        best_params[\"num_epochs\"],\n",
        "        final_model\n",
        "    )\n",
        "\n",
        "    return test_acc\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, out_channels=16, kernel_size=3, hidden_size=64, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1D CNN\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            in_channels=88,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size\n",
        "        )\n",
        "\n",
        "        # Two-layer LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=out_channels,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # FFNN for classification (5 gesture classes)\n",
        "        self.fc = nn.Linear(hidden_size * (64 - kernel_size + 1), 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: batch_size x 88 x 64\n",
        "        x = self.conv1(x)       # batch_size x out_channels x (64 - kernel_size +1)\n",
        "        x = torch.relu(x)\n",
        "        x = x.permute(0, 2, 1) # reshape for LSTM\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.flatten(x)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        try:\n",
        "            np.allclose(logits.shape[1], 5)\n",
        "        except:\n",
        "            print(\"The output of your FFNN is wrong -- should be 5 classes\")\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    device = (\n",
        "        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "    return device\n",
        "\n",
        "\n",
        "class BiosignalDataset(Dataset):\n",
        "    \"\"\"Biosignal dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        temp = pd.read_csv(csv_file)\n",
        "        self.biosignals = temp\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.biosignals)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        biosignal_path = self.biosignals.fname_EMG.iloc[idx]\n",
        "        label = self.biosignals.motionlabel.iloc[idx]\n",
        "        data = pd.read_csv(biosignal_path, header=None, index_col=None).values.T.tolist()\n",
        "        data = torch.tensor(data)\n",
        "        return data, label\n",
        "\n",
        "\n",
        "def vanillaNN(training_data, test_data, batch_size, num_epochs, model):\n",
        "    model = model.to(get_device())\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(torch.float).to(get_device())\n",
        "            labels = labels.to(torch.int64).to(get_device())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Test evaluation\n",
        "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(torch.float).to(get_device())\n",
        "            labels = labels.to(torch.int64).to(get_device())\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_val_acc = correct / total\n",
        "    return model, test_val_acc\n"
      ],
      "metadata": {
        "id": "64MTAZqQNWYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO test your code.\n",
        "# The accuracy should be at least 60%\n",
        "accuracy = deep_learning(train,test)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "iFn6ddUvE0LV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f16e5b0-42d6-4bd6-fa59-5862ec7de4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'out_channels': 8, 'kernel_size': 3, 'hidden_size': 32, 'dropout': 0.0, 'num_epochs': 20, 'batch_size': 2}\n",
            "Accuracy: 80.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# template-matching\n",
        "- [Documentation for PCA function](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
        "\n",
        "hint: You can get the principal component of the fitted PCA by running ``pca.components_``"
      ],
      "metadata": {
        "id": "HI2-k3-GG7I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "template_matching\n",
        "  code skeleton for template_matching algorithm\n",
        "  inputs:\n",
        "      train: training data [class name x data (64 x 88)]\n",
        "      test: testing data [class name x data (64 x 88)]\n",
        "      N_PC: number of principal components (hyperparameter)\n",
        "\"\"\"\n",
        "def template_matching(train,test,N_PC=None):\n",
        "  # import and save training data\n",
        "  templates = []\n",
        "  for file in train:\n",
        "    data = pd.read_csv(file[0], header=None).to_numpy()\n",
        "\n",
        "    # TODO find principal components of template and return the N_PC principal components.\n",
        "    X_pca = get_pca(data,N_PC)\n",
        "    templates.append([file[1],data,X_pca])\n",
        "\n",
        "  # for each test data, find nearest matching training data\n",
        "  test_labels = []\n",
        "  for file in test:\n",
        "    data = pd.read_csv(file[0], header=None).to_numpy()\n",
        "\n",
        "    # TODO go through each template to find one that is the closest distance\n",
        "    # you will need to initialize a minimum distance and gesture label to keep track\n",
        "    # as you go through the templates\n",
        "    min_distance = np.inf\n",
        "    gesture_label = None\n",
        "\n",
        "    for temp_label, temp_data, X_pca in templates:\n",
        "\n",
        "      # TODO for each template, apply PCA to both the train and test data\n",
        "          test_applied = apply_pca(data, X_pca)\n",
        "          train_applied = apply_pca(temp_data, X_pca)\n",
        "\n",
        "          #normalize data --> before it just spiked at 1 at 100%\n",
        "          test_applied_norm  = (test_applied  - np.mean(test_applied,  axis=0)) / (np.std(test_applied,  axis=0) + 1e-8)\n",
        "          train_applied_norm = (train_applied - np.mean(train_applied, axis=0)) / (np.std(train_applied, axis=0) + 1e-8)\n",
        "\n",
        "      # TODO for each template, compute distance between the test and train\n",
        "      # Euclidean distance\n",
        "\n",
        "          distance = np.linalg.norm(test_applied_norm - train_applied_norm)\n",
        "\n",
        "      # TODO for each template, compare distance against min_distance\n",
        "\n",
        "          if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            gesture_label = temp_label\n",
        "\n",
        "    # TODO save test_labels\n",
        "    test_labels.append([file[1],gesture_label])\n",
        "\n",
        "  # TODO calculate accuracy\n",
        "  # accuracy_score documentation: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
        "  # use the documentation to fill in\n",
        "  actual = [t[1] for t in test]\n",
        "  predicted = [tl[1] for tl in test_labels]\n",
        "  test_accuracy = accuracy_score(actual, predicted)\n",
        "\n",
        "  #check that it's at least 60%\n",
        "  #print(f\"Accuracy: {test_accuracy:.2%}\")\n",
        "\n",
        "  return test_accuracy\n",
        "\n",
        "\n",
        "def get_pca(data,N_PC):\n",
        "    # TODO find principal components of template and return the N_PC principal components.\n",
        "    # The output should be an array of shape 88 x N_PC\n",
        "    # hint: You can get the principal component of the fitted PCA by running pca.components_\n",
        "\n",
        "    # TODO this should be 88 x N_PC\n",
        "    pca = PCA(n_components=N_PC)\n",
        "    pca.fit(data)\n",
        "\n",
        "\n",
        "    X_pca = pca.components_.T\n",
        "\n",
        "    # TODO check if this is true, otherwise PCA is implemented incorrectly\n",
        "    try:\n",
        "      np.allclose([88,N_PC],X_pca.shape)\n",
        "    except:\n",
        "      print(\"the Shape of X_pca is wrong, should be 88 x N_PC\")\n",
        "    return X_pca\n",
        "\n",
        "def apply_pca(data,X_pca):\n",
        "  # You can perform matrix multiplication using the np.matmul function\n",
        "  transformed = np.matmul(data, X_pca)\n",
        "\n",
        "  try: # TODO check if this is true, otherwise this code is implemented incorrectly\n",
        "    np.allclose([64,X_pca.shape[1]],transformed.shape)\n",
        "  except:\n",
        "    print(\"The shape of your matrix multiplication is wrong, should be 64 x N_PC\")\n",
        "  return transformed"
      ],
      "metadata": {
        "id": "2tNIVX4qGyee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO test your code.\n",
        "# The accuracy should be at least 60%\n",
        "# TODO choose your N_PC (your hyperparameter)\n",
        "\n",
        "#tuning hyperparamter\n",
        "\n",
        "best_accur = 0\n",
        "best_npc = None\n",
        "for k in range(1,65):\n",
        "  acc = template_matching(train, test, N_PC = k)\n",
        "  if acc > best_accur:\n",
        "    best_accur = acc\n",
        "    best_npc = k\n",
        "\n",
        "N_PC = best_npc\n",
        "accuracy = template_matching(train,test,N_PC=N_PC)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"N_PC: {N_PC}\")\n"
      ],
      "metadata": {
        "id": "WD_yqk5APteO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bcd0ed-f45f-4f98-8982-2d4c7cae1d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 80.00%\n",
            "N_PC: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGpUg6bu9qp2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}